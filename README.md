# Deep Neural Network from Scratch

## Objective
To implement a deep neural network by explicitly coding forward propagation and backward propagation without using high-level deep learning frameworks.

## Description
This experiment demonstrates how multilayer neural networks learn by manually implementing linear transformations, activation functions, loss computation, gradient calculation, and parameter updates using gradient descent. The model processes input through multiple hidden layers and improves predictions iteratively.

## Features
- Forward propagation implementation
- Backward propagation using chain rule
- Gradient descent optimization
- Loss calculation and prediction

## Project Structure
- Assignment.ipynb / Student.ipynb – Main experiment notebook
- dnn_app_utils_v3.py – Helper functions
- datasets/ – Input datasets
- images/ – Supporting images

## Technologies Used
- Python
- NumPy
- Matplotlib
- Jupyter Notebook / Google Colab

## How to Run
1. Open the notebook in Jupyter Notebook or Google Colab.
2. Upload required datasets and utility files.
3. Run all cells sequentially.

## Outcome
The experiment provides a clear understanding of neural network training, gradient flow, and optimization by implementing deep learning concepts from scratch.
